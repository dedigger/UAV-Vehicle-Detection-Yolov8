import tempfile
from pathlib import Path
import numpy as np
import cv2  # opencv-python
from ultralytics import YOLO
import deep_sort.deep_sort.deep_sort as ds
import datetime
import os


def putTextWithBackground(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX,
                          font_scale=1, text_color=(255, 255, 255),
                          bg_color=(0, 0, 0), thickness=1):
    """ç»˜åˆ¶å¸¦æœ‰èƒŒæ™¯çš„æ–‡æœ¬ã€‚"""
    (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)
    bottom_left = origin
    top_right = (origin[0] + text_width, origin[1] - text_height - 5)
    cv2.rectangle(img, bottom_left, top_right, bg_color, -1)
    text_origin = (origin[0], origin[1] - 5)
    cv2.putText(img, text, text_origin, font, font_scale, text_color, thickness, lineType=cv2.LINE_AA)


def extract_detections(results, detect_class):
    """ä»æ¨¡å‹ç»“æœä¸­æå–å’Œå¤„ç†æ£€æµ‹ä¿¡æ¯ã€‚"""
    detections = np.empty((0, 4))
    confarray = []

    for r in results:
        for box in r.boxes:
            if box.cls[0].int() == detect_class:
                x1, y1, x2, y2 = box.xywh[0].int().tolist()
                conf = round(box.conf[0].item(), 2)
                detections = np.vstack((detections, np.array([x1, y1, x2, y2])))
                confarray.append(conf)
    return detections, confarray


def blur_objects(frame, detections):
    """å¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡åŒºåŸŸè¿›è¡Œæ¨¡ç³Šå¤„ç†ã€‚"""
    for (x, y, w, h) in detections:
        x1 = int(x - w / 2)
        y1 = int(y - h / 2)
        x2 = int(x + w / 2)
        y2 = int(y + h / 2)
        roi = frame[y1:y2, x1:x2]
        if roi.size == 0:
            continue
        blurred = cv2.GaussianBlur(roi, (99, 99), 30)
        frame[y1:y2, x1:x2] = blurred
    return frame


def detect_and_track(input_path: str, output_path: str, detect_class: int, model, tracker) -> Path:
    """å¤„ç†è§†é¢‘ï¼Œæ£€æµ‹å¹¶è·Ÿè¸ªç›®æ ‡ã€‚"""

    # åˆ›å»ºå­æ–‡ä»¶å¤¹
    video_dir = Path(output_path) / "videos"
    snapshot_dir = Path(output_path) / "snapshots"
    video_dir.mkdir(parents=True, exist_ok=True)
    snapshot_dir.mkdir(parents=True, exist_ok=True)

    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        print(f"Error opening video file {input_path}")
        return None

    fps = cap.get(cv2.CAP_PROP_FPS)
    size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_video_path = video_dir / f"output_{timestamp}.avi"

    fourcc = cv2.VideoWriter_fourcc(*"XVID")
    output_video = cv2.VideoWriter(str(output_video_path), fourcc, fps, size, isColor=True)

    while True:
        success, frame = cap.read()
        if not success:
            break

        results = model(frame, stream=True)
        detections, confarray = extract_detections(results, detect_class)
        resultsTracker = tracker.update(detections, confarray, frame)

        for i, (x1, y1, x2, y2, Id) in enumerate(resultsTracker):
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])

            # ç»˜åˆ¶ç›®æ ‡æ¡†
            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 3)

            # æ„å»ºæ ‡ç­¾å­—ç¬¦ä¸²ï¼šç±»åˆ« + ç½®ä¿¡åº¦
            conf = confarray[i] if i < len(confarray) else 0.0
            label = f"{model.names[detect_class]} {conf:.2f}"

            # ç»˜åˆ¶ç±»åˆ«ä¸IDæ–‡æœ¬
            putTextWithBackground(frame, label, (x1, y1), font_scale=0.8, bg_color=(0, 0, 0))
            putTextWithBackground(frame, f"ID: {int(Id)}", (x1, y1 + 30), font_scale=0.8, bg_color=(255, 0, 255))

        output_video.write(frame)
        cv2.imshow("Detection and Tracking", frame)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            print("Stopped by user.")
            break
        elif key == ord('s'):
            # æŒ‰ä¸‹ s é”®ï¼Œä¿å­˜æ‰“ç å›¾åƒåˆ° snapshot_dir
            save_frame = frame.copy()
            blurred_frame = blur_objects(save_frame, detections)
            image_filename = snapshot_dir / f"snapshot_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
            cv2.imwrite(str(image_filename), blurred_frame)
            print(f"âœ… Frame saved and blurred at: {image_filename}")

    output_video.release()
    cap.release()
    cv2.destroyAllWindows()

    print(f'ğŸ“¹ Output video saved at: {output_video_path}')
    return output_video_path


if __name__ == "__main__":
    input_path = r"D:\YOLO-World-master\YOLO-World-master\3333.mp4"
    output_path = r"D:\YOLO-World-master\YOLO-World-master"

    # åŠ è½½æ¨¡å‹
    model = YOLO(r"D:\YOLO-World-master\YOLO-World-master\yolov8n (1).pt")
    detect_class = 0  # 'person'
    print(f"Detecting: {model.names[detect_class]}")

    # DeepSort åˆå§‹åŒ–
    tracker = ds.DeepSort("deep_sort/deep_sort/deep/checkpoint/ckpt.t7")

    detect_and_track(input_path, output_path, detect_class, model, tracker)
#https://github.com/KdaiP/yolov8-deepsort-tracking